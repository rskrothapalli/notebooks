{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Fun with AI: LangChain and GPT-3.5 Turbo Unleashed. Meet Kangala!'\n",
        "author: Ravi Sankar Krothapalli\n",
        "date: '2023-10-05'\n",
        "toc: true\n",
        "format:\n",
        "  html:\n",
        "    html-math-method: katex\n",
        "    code-tools: false\n",
        "    self-contained: true\n",
        "execute:\n",
        "  enabled: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction to Langchain\n",
        "\n",
        "In this blog post, we will explore the fundamentals of LangChain, highlighting some of its key features and demonstrating how it can facilitate the development of intelligent applications. From setting up your environment to creating engaging content, LangChain simplifies the entire process, making it both straightforward and enjoyable. Join me on this exciting journey and discover how LangChain can elevate your AI projects to the next level!\n",
        "\n",
        "### Setup the environment\n",
        "\n",
        "To use the OpenAI API and other services securely, you need to create a `.env` file in the root directory of your project. This file will store your API keys and other sensitive information.\n",
        "\n",
        "Add the following environment variable to the `.env` file:\n",
        "\n",
        "> OPENAI_API_KEY=your_openai_api_key_here\n",
        "\n",
        "By following these steps, you can securely manage your environment variables and keep your sensitive information safe.\n",
        "\n",
        "::: callout-note\n",
        "**Before proceeding, please make sure to install the following libraries**:\n",
        "\n",
        "```         \n",
        "ipykernel\n",
        "jupyter\n",
        "langchain\n",
        "langchain-community\n",
        "langchain-openai\n",
        "langgraph\n",
        "nbclient\n",
        "openai\n",
        "python-dotenv\n",
        "pyyaml\n",
        "```\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "if loading_envs := load_dotenv():\n",
        "    print(\"Loaded environment variables\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "### Generating fun facts about animals with OpenAI's GPT-3.5 Turbo\n",
        "\n",
        "Now, let's have some fun with OpenAI's GPT-3.5 Turbo model. We're going to generate some hilarious animal facts that will make you giggle! ðŸ¦¥\n",
        "\n",
        "The `chat.completions.create` endpoint is used to generate responses from the model based on a given prompt.\n",
        "\n",
        "This endpoint allows you to:\n",
        "\n",
        "-   Specify the model (e.g., GPT-3.5 Turbo)\n",
        "\n",
        "-   Provide messages, including system and user messages\n",
        "\n",
        "-   Tailor the response with various parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "from openai import AsyncOpenAI\n",
        "\n",
        "llm_model = AsyncOpenAI()\n",
        "\n",
        "# Invoke the model with the prompt and print the response\n",
        "response = llm_model.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant. \n",
        "                                        Your purpose is to share fun facts with a 5 year old.\n",
        "                                        Provide a friendly response and make it funny. \n",
        "                                        Make sure each sentence appears in a new line\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me 2 fun facts about sloths.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm_model_response = await response\n",
        "\n",
        "print(llm_model_response.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "### Using LangChain to generate fun facts about animals\n",
        "\n",
        "LangChain makes working with language models even easier. It's like having a magic wand that simplifies everything! ðŸª„\n",
        "\n",
        "The following code snippet demonstrates how to use LangChain's `ChatOpenAI` model to generate a to generate fun facts about animals.\n",
        "\n",
        "Following are the some of the features of Langchain:\n",
        "\n",
        "-   **Higher-level abstraction**: Simplifies working with language models.\n",
        "\n",
        "-   **Complex workflows**: Easier management and integration with other tools.\n",
        "\n",
        "-   **Conversational contexts**: Handles contexts effectively.\n",
        "\n",
        "-   **Time-saving**: Reduces boilerplate code compared to direct OpenAI API usage.\n",
        "\n",
        "#### **Initialize the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "fun_facts_chat_model = init_chat_model(\"gpt-3.5-turbo\", model_provider=\"openai\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Crafting the chat prompt message and invoking the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"\"\"You are a helpful assistant. \n",
        "                Your purpose is to share fun facts with a 5 year old.\n",
        "                Provide a friendly response and make it funny. \n",
        "                Make sure each sentence appears in a new line\"\"\"),\n",
        "    HumanMessage(\"Tell me 2 fun facts about sloths.\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with the created prompt and print the response\n",
        "response = fun_facts_chat_model.invoke(messages)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Generating prompt using Prompt Templates**\n",
        "\n",
        "Creating effective prompts can be streamlined with Prompt Templates. These templates allow you to define reusable and dynamic prompts with placeholders, making them adaptable for various scenarios.\n",
        "\n",
        "At runtime, these placeholders are replaced with actual values, ensuring your prompts are always relevant and up-to-date. This method ensures consistent and flexible prompt creation, maintaining a structured and efficient process.\n",
        "\n",
        "By using Prompt Templates, you can create prompts that are both consistent and adaptable, making your workflow smoother and more efficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a helpful assistant. \n",
        "                Your purpose is to share fun facts with a 5 year old.\n",
        "                Provide a friendly response and make it funny. \n",
        "                Make sure each sentence appears in a new line\"\"\"),\n",
        "    (\"user\", \"Tell me 2 fun facts about {animal}.\")\n",
        "])\n",
        "\n",
        "# Create a prompt with the specified animal\n",
        "prompt = prompt_template.invoke({\"animal\": \"koalas\"})\n",
        "print(f\"printing updated prompt: {prompt.to_messages()[1].content}\\n\")\n",
        "\n",
        "# Invoke the model with the created prompt and print the response\n",
        "response = fun_facts_chat_model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "### Simple RAG Application\n",
        "\n",
        "Finally, let's build a simple Retrieval Augmented Generation (RAG) application.\n",
        "\n",
        "**`Retrieval Augmented Generation (RAG)`** is a technique that combines document retrieval with language generation to produce accurate and contextually relevant responses.\n",
        "\n",
        "Let's set the stage to demonstrate the power of RAG.\n",
        "\n",
        "**Meet Kangala:**\n",
        "\n",
        "<figure>\n",
        "\n",
        "::: {style=\"text-align: center;\"}\n",
        "<img src=\"kangala.jpeg\" alt=\"Alt text\" width=\"300\" height=\"300\"/>\n",
        ":::\n",
        "\n",
        "<figcaption>generated using copilot</figcaption>\n",
        "\n",
        "</figure>\n",
        "\n",
        "The Kangala is a whimsical creature with rainbow-colored fur that sparkles in the sunlight and a playful expression that brings joy to everyone who sees it.\n",
        "\n",
        "Known for its ability to make flowers bloom wherever it goes, the Kangala loves to jump around and play, making it a delightful companion in any magical forest! ðŸŒˆðŸ¦„\n",
        "\n",
        "**Why RAG?:**\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is key to creating engaging content about the whimsical Kangala. By combining document retrieval with language generation, RAG ensures that fun facts are both accurate and contextually relevant. This approach allows the application to deliver delightful and informative responses, making the Kangala come alive in a magical and entertaining way.\n",
        "\n",
        "Here's how you can create fun facts about the Kangala:\n",
        "\n",
        "-   **Document Creation**: Store fun facts about the Kangala in an in-memory vector store.\n",
        "\n",
        "-   **Embedding Generation**: Use OpenAIâ€™s embedding model to create numerical representations of the documents.\n",
        "\n",
        "-   **Similarity Search**: Retrieve the most relevant documents based on a query.\n",
        "\n",
        "-   **Prompt Construction**: Construct a prompt using the retrieved documents.\n",
        "\n",
        "-   Response Generation: Generate a response from the language model.\n",
        "\n",
        "#### **Document creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Kangalas have bright, rainbow-colored fur that sparkles in the sunlight.\",\n",
        "        metadata={\"source\": \"imaginary-animals-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Kangalas can make funny, musical sounds that make everyone laugh and dance.\",\n",
        "        metadata={\"source\": \"imaginary-animals-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Kangalas love to eat sweet fruits and berries, especially magical starberries.\",\n",
        "        metadata={\"source\": \"imaginary-animals-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Kangalas can jump really high, almost like they have springs in their legs, and they love to play leapfrog.\",\n",
        "        metadata={\"source\": \"imaginary-animals-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Kangalas can change the color of their fur to match their surroundings, just like a chameleon, making them great at hide and seek.\",\n",
        "        metadata={\"source\": \"imaginary-animals-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Kangalas have a magical ability to make flowers bloom wherever they go, turning the dry lands into a colorful garden.\",\n",
        "        metadata={\"source\": \"imaginary-animals-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Similarity Search**\n",
        "\n",
        "**Embeddings**\n",
        "\n",
        ":   `Embeddings` are a way to represent words, phrases, or even entire documents as vectors (arrays of numbers) in a continuous vector space. This allows machines to understand and process natural language in a more meaningful way.\n",
        "\n",
        "In our use case, Embeddings help capture the semantic meaning of documents and their relationships to each other, which helpful in information retrieval.\n",
        "\n",
        "Imagine a catalog where each document is a puzzle piece. Document-level embeddings transform each document into a unique shape that fits perfectly with related documents. When you search for a specific topic, it's like finding the right pieces that fit together to form a coherent picture. This helps in quickly retrieving the most relevant documents, just as assembling the right puzzle pieces reveals the complete image.\n",
        "\n",
        "`OpenAIEmbeddings` allows you to create embeddings for text using OpenAI's models. Embeddings are numerical representations of text that capture the semantic meaning, making it easier to perform tasks like similarity searches.\n",
        "\n",
        "`InMemoryVectorStore` provides an in-memory storage solution for vectors (embeddings). It allows you to add documents, store their embeddings, and perform similarity searches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# Initialize OpenAI embeddings with the specified model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# Create an in-memory vector store to hold the embeddings\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "\n",
        "# Add the documents to the vector store\n",
        "_ = vector_store.add_documents(documents=documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Construct prompt and generate a response**\n",
        "\n",
        "This code sets up a simple application using LangChain and LangGraph frameworks. It defines a state that includes an animal name, a context of documents, and an answer.\n",
        "\n",
        "The application has two main functions:\n",
        "\n",
        "-   Retrieve: This function searches for documents related to the given animal and updates the context with these documents.\n",
        "\n",
        "-   Generate: This function uses the context to generate an answer based on a predefined prompt template and a chat model.\n",
        "\n",
        "The application is then compiled into a state graph, which defines the sequence of operations. Finally, the graph is executed with an initial state, and the generated answer is printed. This setup allows for efficient retrieval and generation of information based on the given input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "\n",
        "prompt_template.append(\n",
        "    {\"role\": \"system\", \"content\": \"Context: {context} \\n Answer: \"})\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    animal: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"animal\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt_template.invoke(\n",
        "        {\"animal\": state[\"animal\"], \"context\": docs_content})\n",
        "    response = fun_facts_chat_model.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "response = graph.invoke({\"animal\": \"Kangala\"})\n",
        "print(response[\"answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/KROTHRX2/arraysoflight/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}